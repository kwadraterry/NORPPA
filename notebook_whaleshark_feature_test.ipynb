{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup autoreload, warnings and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, Markdown\n",
    "def print_heading(string):\n",
    "    display(Markdown(f\"# {string}\"))\n",
    "def print_subheading(string):\n",
    "    display(Markdown(f\"## {string}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the visibility of cuda devices (in case your system contains more than one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ../..\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(x, file):\n",
    "    with open(file, 'wb') as f_file:\n",
    "        pickle.dump(x, f_file, protocol=4)\n",
    "        \n",
    "def load_pickle(file):\n",
    "    with open(file, 'rb') as f_file:\n",
    "        result = pickle.load(f_file)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ekaterina/env/norppa/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 10:19:21.373690: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-14 10:19:22.947716: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-14 10:19:22.993193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-06-14 10:19:22.993236: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-14 10:19:22.996457: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-06-14 10:19:22.996518: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-06-14 10:19:22.997575: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-14 10:19:22.997841: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-14 10:19:23.001059: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-06-14 10:19:23.001875: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-06-14 10:19:23.002037: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-14 10:19:23.003035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from config_whaleshark import config\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import wget\n",
    "import pickle\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from torchvision.datasets.utils import download_url\n",
    "from datasets import SimpleDataset, DatasetSlice\n",
    "\n",
    "from tools import apply_pipeline, crop_step, curry, curry_sequential, apply_pipeline_dataset, get_save_step, apply_sequential, compose, compose_sequential\n",
    "from tonemapping.tonemapping import tonemap, tonemap_step\n",
    "from segmentation.segmentation import segment\n",
    "from pattern_extraction.extract_pattern import extract_pattern\n",
    "from pattern_extraction.extract_pattern import smart_resize\n",
    "from reidentification.identify import encode_single, encode_pipeline, encode_dataset, identify, identify_single, apply_geometric, encode_patches, extract_patches\n",
    "from reidentification.visualisation import visualise_match\n",
    "from reidentification.find_matches import find_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a configuration file.\n",
    "You can change the default parameters in config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"topk\"]=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_dataset(input, size):\n",
    "    image, img_label = input\n",
    "    if image is None:\n",
    "        return [input]\n",
    "\n",
    "    result, ratio = smart_resize(image, size, return_ratio=True)\n",
    "    img_label[\"resize_ratio\"] = ratio\n",
    "    return [(result, img_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_matrix(identification_result):\n",
    "    result = []\n",
    "    for (db_labels, q_labels) in identification_result:\n",
    "        q_class = q_labels['class_id']\n",
    "        q_ln = len(q_labels['labels'])\n",
    "        result.append([db_label['db_label']['class_id']==q_class for db_label in db_labels]*q_ln)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "def get_topk_accuracy(identification_result):\n",
    "    result = []\n",
    "    for (db_labels, q_labels) in identification_result:\n",
    "        q_class = q_labels['class_id']\n",
    "        q_ln = len(q_labels['labels'])\n",
    "        result.append([db_label['db_label']['class_id']==q_class for db_label in db_labels]*q_ln)\n",
    "    result = np.asarray(result)\n",
    "    return [sum((np.sum(result[:, :j+1], axis=1) > 0)) / len(result) for j in range(result.shape[1])]\n",
    "\n",
    "def print_topk_accuracy(identification_result, label=\"\"):\n",
    "    topk_acc = get_topk_accuracy(identification_result)\n",
    "    print(label)\n",
    "    for (i, acc) in enumerate(topk_acc):\n",
    "        print(f\"Top-{i+1} accuracy: {acc*100}%\")\n",
    "    return identification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "def deaffinize_laf(laf):\n",
    "    xy = KF.laf.get_laf_center(laf)\n",
    "    sc = KF.laf.get_laf_scale(laf)\n",
    "    ori = KF.laf.get_laf_orientation(laf)\n",
    "    return KF.laf.laf_from_center_scale_ori(xy, sc, ori)\n",
    "\n",
    "def get_lafs_and_descriptors(img, harrisz_kpts,\n",
    "                             kornia_descriptor,\n",
    "                             ori,\n",
    "                             aff,\n",
    "                             aff_string = 'def',\n",
    "                             mrSize=1.0,\n",
    "                             num_feats = 2048, device=torch.device('cuda')):\n",
    "    # We will not train anything, so let's save time and memory by no_grad()\n",
    "    with torch.no_grad():\n",
    "        timg = K.image_to_tensor(img, False).float()/255.\n",
    "        timg = timg.to(device)\n",
    "#         timg = K.color.rgb_to_grayscale(timg)\n",
    "        # timg = K.color.rgb_to_grayscale(K.image_to_tensor(img, False))/255.\n",
    "        # timg = timg.to(device)\n",
    "        hz_pt = torch.from_numpy(harrisz_kpts).float()\n",
    "        lafs = KF.laf.ellipse_to_laf(hz_pt[None])\n",
    "        lafs[0,:,0,0] = hz_pt[:,2] * mrSize\n",
    "        lafs[0,:,0,1] = hz_pt[:,3] * mrSize\n",
    "        lafs[0,:,1,0] = hz_pt[:,3] * 0\n",
    "        lafs[0,:,1,1] = hz_pt[:,4] * mrSize\n",
    "        lafs = lafs.to(timg.device)\n",
    "        # border image changed from 5 to 10 !!!\n",
    "        good_lafs_mask = KF.laf.laf_is_inside_image(lafs, timg, 10)\n",
    "        # all taken!!!\n",
    "        # good_lafs_mask[:] = True \n",
    "        good_lafs = lafs[good_lafs_mask][None]\n",
    "        if aff_string == 'def':\n",
    "            out_lafs = good_lafs\n",
    "        elif aff_string == 'no':\n",
    "            out_lafs = deaffinize_laf(good_lafs)\n",
    "        elif aff_string == 'affnet':\n",
    "            out_lafs = deaffinize_laf(good_lafs)\n",
    "            out_lafs = aff(out_lafs,timg)\n",
    "        elif aff_string == 'orinet_affnet':\n",
    "            out_lafs = deaffinize_laf(good_lafs)\n",
    "            out_lafs = ori(aff(out_lafs,timg),timg)\n",
    "        else:\n",
    "            raise ValueError('Unknown affine str')\n",
    "        out_lafs = out_lafs[:,:num_feats]\n",
    "        # We will estimate affine shape of the feature and re-orient the keypoints with the OriNet\n",
    "        patches = KF.extract_patches_from_pyramid(timg,out_lafs, 32)\n",
    "        B, N, CH, H, W = patches.size()\n",
    "        # Descriptor accepts standard tensor [B, CH, H, W], while patches are [B, N, CH, H, W] shape\n",
    "        # So we need to reshape a bit :) \n",
    "        descs = kornia_descriptor(patches.view(B * N, CH, H, W)).view(B * N, -1)\n",
    "    return out_lafs, descs.detach().cpu().numpy()#, good_lafs_mask[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 99/99 [00:01<00:00, 50.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/2 steps\n",
      "Calculating PCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ekaterina/work/src/NORPPA/repository/NORPPA/reidentification/identify.py:225: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_ells = np.array(all_ells)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting encoding parameters...\n",
      "Encoding...\n",
      "Completed 2/2 steps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from extract_patches.laf import LAFs2ell\n",
    "from extract_patches.core import extract_patches as keypoints_to_patches\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy.io\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# f = h5py.File('/ekaterina/work/src/NORPPA/repository/NORPPA/temp/files/result_pattern_tonemapped_wrong.mat','r')\n",
    "# data = f.get('data/result')\n",
    "# data = np.array(data)\n",
    "\n",
    "train_path = \"/ekaterina/work/data/whaleshark_wrong_tonemapped_mini\"\n",
    "# train_path = \"/ekaterina/work/data/whaleshark_norppa_tonemapped_resized/train\"\n",
    "dataset_train = Path(train_path)\n",
    "train_dataset = SimpleDataset(dataset_train)\n",
    "\n",
    "\n",
    "def get_class_and_name(full_path):\n",
    "    head, file = os.path.split(full_path)\n",
    "    _, class_name = os.path.split(head)\n",
    "    return os.path.join(class_name, file)\n",
    "\n",
    "def cell_to_dict(mat):\n",
    "    res = {}\n",
    "    for i in range(mat.shape[0]):\n",
    "        res[os.path.join(mat[i, 0][0], mat[i, 1][0])] = mat[i, 2]\n",
    "    return res\n",
    "\n",
    "\n",
    "def patchify_load(dataset, file, config):\n",
    "    mat = scipy.io.loadmat(file)\n",
    "    matdict = cell_to_dict(mat['result'])\n",
    "    \n",
    "    net = config[\"net\"]\n",
    "    result = []\n",
    "    labels = []\n",
    "    inds = []\n",
    "    all_ells = []\n",
    "    ind = 0\n",
    "    num_files = len(dataset)\n",
    "    dataset_transforms = transforms.Grayscale(num_output_channels=1)\n",
    "    \n",
    "    if config['use_cuda']:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    PS = 32\n",
    "    descriptor = KF.HardNet(True)\n",
    "#     descriptor = KF.HardNet8(True)        \n",
    "    \n",
    "    descriptor = descriptor.to(device)\n",
    "    # print (device)\n",
    "    descriptor.eval()\n",
    "    aff_est = KF.LAFAffNetShapeEstimator(True).to(device)\n",
    "    orienter = KF.LAFOrienter(32, angle_detector=KF.OriNet(True)).to(device)\n",
    "    orienter.eval()\n",
    "    aff_est.eval()\n",
    "    NUM_KP = 500\n",
    "    mrSize = 0.5\n",
    "    \n",
    "    for i, (image, img_label) in enumerate(tqdm(dataset)):\n",
    "        if image is None:\n",
    "            all_ells.append(None)\n",
    "            labels.append(img_label)\n",
    "            continue\n",
    "        image = dataset_transforms(image)\n",
    "        num_files-=1\n",
    "        if sum(image.getextrema()) == 0:\n",
    "            all_ells.append(None)\n",
    "            labels.append(img_label)\n",
    "            continue\n",
    "#         patches, ells = patch_extraction(image, config)\n",
    "        key = get_class_and_name(img_label['file'])\n",
    "        if key not in matdict:\n",
    "            continue\n",
    "\n",
    "        hz_kpts = matdict[key]\n",
    "        \n",
    "        lafs, descs = get_lafs_and_descriptors(np.array(image), hz_kpts,\n",
    "                     descriptor,\n",
    "                     orienter,\n",
    "                     aff_est,\n",
    "                     aff_string = \"affnet\", # \"orinet_affnet\", # \n",
    "                     mrSize=mrSize,\n",
    "                     num_feats = NUM_KP, device=device)\n",
    "        kps_back = opencv_kpts_from_laf(lafs, mrSize)\n",
    "#         out_img = cv2.drawKeypoints(np.array(image), kps_back, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#         plt.imshow(out_img)\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(descs.shape)\n",
    "        patch_features = descs\n",
    "#         ells = LAFs2ell(np.array(lafs[0].cpu()))\n",
    "        ells = np.array([[kp.pt[0], kp.pt[1], kp.size, kp.size, kp.angle] for kp in kps_back])\n",
    "        \n",
    "\n",
    "        all_ells.append(ells)\n",
    "        inds.extend([i] * patch_features.shape[0])\n",
    "        labels.append(img_label)\n",
    "        result.append(patch_features)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    return np.vstack(result), np.array(inds), labels, all_ells\n",
    "\n",
    "def extract_patches_load(dataset, file, config):\n",
    "    return (dataset, patchify_load(dataset, file, config))\n",
    "\n",
    "keypoints_path = '/ekaterina/work/src/NORPPA/repository/NORPPA/temp/files/result_tonemapped_wrong.mat'\n",
    "\n",
    "\n",
    "def patchify_test(dataset, config):\n",
    "    net = config[\"net\"]\n",
    "    result = []\n",
    "    labels = []\n",
    "    inds = []\n",
    "    all_ells = []\n",
    "    ind = 0\n",
    "    num_files = len(dataset)\n",
    "    dataset_transforms = transforms.Grayscale(num_output_channels=1)\n",
    "    \n",
    "    if config['use_cuda']:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "#     descriptor = KF.HardNet8(True)        \n",
    "    mrSize = 1.0\n",
    "#     detector = KF.KeyNetAffNetHardNet(num_features=5000, upright=False, device=device, scale_laf=mrSize)\n",
    "    detector = KF.DISK.from_pretrained('depth', device=device)\n",
    "    for i, (image, img_label) in enumerate(tqdm(dataset)):\n",
    "        if image is None:\n",
    "            all_ells.append(None)\n",
    "            labels.append(img_label)\n",
    "            continue\n",
    "#         image = dataset_transforms(image)\n",
    "        num_files-=1\n",
    "#         if sum(image.getextrema()) == 0:\n",
    "#             all_ells.append(None)\n",
    "#             labels.append(img_label)\n",
    "#             continue\n",
    "#         image = np.array(image)[None, :, :, None]\n",
    "        image = np.array(image)[None, :, :]\n",
    "        timg = K.image_to_tensor(image, False).float()/255.\n",
    "        timg = timg.to(device)\n",
    "        \n",
    "#         lafs, _, descs = detector(timg)        \n",
    "\n",
    "#         kps_back = opencv_kpts_from_laf(lafs, mrSize)\n",
    "#         patch_features = descs.cpu()[0, ]\n",
    "#         ells = np.array([[kp.pt[0], kp.pt[1], kp.size, kp.size, kp.angle] for kp in kps_back])\n",
    "        \n",
    "        disk = detector(timg, pad_if_not_divisible=True)[0]\n",
    "        patch_features = disk.descriptors.cpu().numpy()\n",
    "        pts = disk.keypoints.cpu().numpy()\n",
    "        ells = np.array([[kp[0], kp[1], 5, 5, 0] for kp in pts])\n",
    "        \n",
    "\n",
    "        all_ells.append(ells)\n",
    "        inds.extend([i] * patch_features.shape[0])\n",
    "        labels.append(img_label)\n",
    "        result.append(patch_features)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    return np.vstack(result), np.array(inds), labels, all_ells\n",
    "def extract_patches_test(dataset, config):\n",
    "    return (dataset, patchify_test(dataset, config))\n",
    "\n",
    "pipeline = [\n",
    "#     curry(extract_patches_load, file=keypoints_path, config=cfg),\n",
    "    curry(extract_patches_test, config=cfg),\n",
    "    curry(encode_patches, cfg=cfg)\n",
    "]\n",
    "\n",
    "encoded_train_dataset = apply_pipeline_dataset(train_dataset, pipeline, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 99/99 [00:00<00:00, 25488.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before geometric verification:\n",
      "Top-1 accuracy: 13.131313131313133%\n",
      "Top-2 accuracy: 28.28282828282828%\n",
      "Top-3 accuracy: 37.37373737373738%\n",
      "Top-4 accuracy: 42.42424242424242%\n",
      "Top-5 accuracy: 48.484848484848484%\n",
      "Top-6 accuracy: 57.57575757575758%\n",
      "Top-7 accuracy: 60.60606060606061%\n",
      "Top-8 accuracy: 61.61616161616161%\n",
      "Top-9 accuracy: 64.64646464646465%\n",
      "Top-10 accuracy: 69.6969696969697%\n",
      "Top-11 accuracy: 74.74747474747475%\n",
      "Top-12 accuracy: 76.76767676767676%\n",
      "Top-13 accuracy: 81.81818181818183%\n",
      "Top-14 accuracy: 84.84848484848484%\n",
      "Top-15 accuracy: 85.85858585858585%\n",
      "Top-16 accuracy: 85.85858585858585%\n",
      "Top-17 accuracy: 85.85858585858585%\n",
      "Top-18 accuracy: 86.86868686868688%\n",
      "Top-19 accuracy: 89.8989898989899%\n",
      "Top-20 accuracy: 89.8989898989899%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_pipeline1 = [\n",
    "                 curry(identify, encoded_train_dataset, cfg[\"topk\"], leave_one_out=True),\n",
    "                 curry(print_topk_accuracy, label=\"Before geometric verification:\"),\n",
    "                ]\n",
    "test_pipeline2 = [\n",
    "                 curry_sequential(find_matches, cfg),\n",
    "                 curry_sequential(apply_geometric, cfg[\"geometric\"]),\n",
    "                 curry(print_topk_accuracy, label=\"After geometric verification:\"),\n",
    "#                  curry_sequential(visualise_match, cfg[\"topk\"])\n",
    "                ]\n",
    "\n",
    "matches1 = apply_pipeline_dataset(encoded_train_dataset, test_pipeline1)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2 = apply_pipeline_dataset(matches1, test_pipeline2)\n",
    "\n",
    "# with open(\"tonemapped_pattern_whaleshark_scale2_matches.pickle\", 'wb') as f_file:\n",
    "#     pickle.dump(codebooks, f_file, protocol=4)\n",
    "# None\n",
    "\n",
    "# Scale 2\n",
    "# Top-1 accuracy: 31.0%\n",
    "# Top-2 accuracy: 49.0%\n",
    "# Top-3 accuracy: 55.00000000000001%\n",
    "# Top-4 accuracy: 59.0%\n",
    "# Top-5 accuracy: 66.0%\n",
    "\n",
    "# Scale 2 (no resize)\n",
    "# Top-1 accuracy: 26.0%\n",
    "# Top-2 accuracy: 43.0%\n",
    "# Top-3 accuracy: 55.00000000000001%\n",
    "# Top-4 accuracy: 61.0%\n",
    "# Top-5 accuracy: 68.0%\n",
    "\n",
    "# Scale 1\n",
    "# Top-1 accuracy: 22.0%\n",
    "# Top-2 accuracy: 26.0%\n",
    "# Top-3 accuracy: 42.0%\n",
    "# Top-4 accuracy: 49.0%\n",
    "# Top-5 accuracy: 52.0%\n",
    "\n",
    "# Scale 3\n",
    "# Top-1 accuracy: 26.0%\n",
    "# Top-2 accuracy: 41.0%\n",
    "# Top-3 accuracy: 53.0%\n",
    "# Top-4 accuracy: 62.0%\n",
    "# Top-5 accuracy: 70.0%\n",
    "\n",
    "\n",
    "# Scale 1\n",
    "# Before geometric verification:\n",
    "# Top-1 accuracy: 27.0%\n",
    "# Top-2 accuracy: 41.0%\n",
    "# Top-3 accuracy: 48.0%\n",
    "# Top-4 accuracy: 55.00000000000001%\n",
    "# Top-5 accuracy: 56.00000000000001%\n",
    "\n",
    "# After geometric verification:\n",
    "# Top-1 accuracy: 42.0%\n",
    "# Top-2 accuracy: 51.0%\n",
    "# Top-3 accuracy: 61.0%\n",
    "# Top-4 accuracy: 66.0%\n",
    "# Top-5 accuracy: 71.0%\n",
    "\n",
    "# Scale 1 tonemapped + tonemapped codebooks\n",
    "\n",
    "# Before GV\n",
    "# Top-1 accuracy: 41.0%\n",
    "# Top-2 accuracy: 55.00000000000001%\n",
    "# Top-3 accuracy: 57.99999999999999%\n",
    "# Top-4 accuracy: 63.0%\n",
    "# Top-5 accuracy: 71.0%\n",
    "\n",
    "# After GV\n",
    "# Top-1 accuracy: 47.0%\n",
    "# Top-2 accuracy: 59.0%\n",
    "# Top-3 accuracy: 64.0%\n",
    "# Top-4 accuracy: 69.0%\n",
    "# Top-5 accuracy: 73.0%\n",
    "\n",
    "# KeyNetAffNetHardNet tonemapped mini\n",
    "\n",
    "# Before GV\n",
    "# Top-1 accuracy: 19.19191919191919%\n",
    "# Top-2 accuracy: 27.27272727272727%\n",
    "# Top-3 accuracy: 35.35353535353536%\n",
    "# Top-4 accuracy: 42.42424242424242%\n",
    "# Top-5 accuracy: 48.484848484848484%\n",
    "\n",
    "# After GV\n",
    "# Top-1 accuracy: 36.36363636363637%\n",
    "# Top-2 accuracy: 43.43434343434344%\n",
    "# Top-3 accuracy: 53.535353535353536%\n",
    "# Top-4 accuracy: 57.57575757575758%\n",
    "# Top-5 accuracy: 59.59595959595959%\n",
    "\n",
    "\n",
    "# HarrisZ+ mini tonemapped\n",
    "\n",
    "# Before GV\n",
    "# Top-1 accuracy: 23.232323232323232%\n",
    "# Top-2 accuracy: 31.313131313131315%\n",
    "# Top-3 accuracy: 43.43434343434344%\n",
    "# Top-4 accuracy: 49.494949494949495%\n",
    "# Top-5 accuracy: 51.515151515151516%\n",
    "\n",
    "# After GV\n",
    "# Top-1 accuracy: 24.242424242424242%\n",
    "# Top-2 accuracy: 34.34343434343434%\n",
    "# Top-3 accuracy: 42.42424242424242%\n",
    "# Top-4 accuracy: 47.474747474747474%\n",
    "# Top-5 accuracy: 55.55555555555556%\n",
    "\n",
    "# HarrisZ+ full tonemapped\n",
    "\n",
    "# Before GV\n",
    "# Top-1 accuracy: 36.14346459604363%\n",
    "# Top-2 accuracy: 42.42928452579035%\n",
    "# Top-3 accuracy: 46.12682566093548%\n",
    "# Top-4 accuracy: 48.73359216121279%\n",
    "# Top-5 accuracy: 51.02606766500277%\n",
    "\n",
    "# After GV\n",
    "\n",
    "# Top-1 accuracy: 47.402477352560545%\n",
    "# Top-2 accuracy: 52.09835459419486%\n",
    "# Top-3 accuracy: 54.85302273987798%\n",
    "# Top-4 accuracy: 56.83120724718063%\n",
    "# Top-5 accuracy: 58.402662229617306%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curry_sequential(visualise_match, 5)(matches2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of torch failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/__init__.py\", line 22, in <module>\n",
      "    from ._utils import _import_dotted_name, classproperty\n",
      "ImportError: cannot import name 'classproperty' from 'torch._utils' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_utils.py)\n",
      "]\n",
      "[autoreload of torch.torch_version failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __gt__() requires a code object with 1 free vars, not 0\n",
      "]\n",
      "[autoreload of torch._tensor failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_tensor.py\", line 84, in <module>\n",
      "    class Tensor(torch._C._TensorBase):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_tensor.py\", line 488, in Tensor\n",
      "    \"\"\")\n",
      "RuntimeError: method 'detach' already has a docstring\n",
      "]\n",
      "[autoreload of torch.overrides failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/overrides.py\", line 33, in <module>\n",
      "    from torch._C import (\n",
      "ImportError: cannot import name '_set_torch_function_mode' from 'torch._C' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "]\n",
      "[autoreload of torch.storage failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/storage.py\", line 214, in <module>\n",
      "    class _UntypedStorage(torch._C.StorageBase, _StorageBase):\n",
      "AttributeError: module 'torch._C' has no attribute 'StorageBase'\n",
      "]\n",
      "[autoreload of torch.serialization failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/serialization.py\", line 16, in <module>\n",
      "    from torch.storage import _get_dtype_from_pickle_storage_type\n",
      "ImportError: cannot import name '_get_dtype_from_pickle_storage_type' from 'torch.storage' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/storage.py)\n",
      "]\n",
      "[autoreload of torch.cuda failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 21, in <module>\n",
      "    from .graphs import CUDAGraph, graph_pool_handle, graph, \\\n",
      "ImportError: cannot import name 'is_current_stream_capturing' from 'torch.cuda.graphs' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/cuda/graphs.py)\n",
      "]\n",
      "[autoreload of torch.cuda.graphs failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/cuda/graphs.py\", line 15, in <module>\n",
      "    from torch._C import _cuda_isCurrentStreamCapturing\n",
      "ImportError: cannot import name '_cuda_isCurrentStreamCapturing' from 'torch._C' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "]\n",
      "[autoreload of torch.cuda.amp.autocast_mode failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py\", line 13, in <module>\n",
      "    class autocast(torch.amp.autocast_mode.autocast):\n",
      "AttributeError: module 'torch' has no attribute 'amp'\n",
      "]\n",
      "[autoreload of torch.sparse failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/sparse/__init__.py\", line 5, in <module>\n",
      "    from torch._C import _add_docstr, _sparse  # type: ignore[attr-defined]\n",
      "ImportError: cannot import name '_sparse' from 'torch._C' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "]\n",
      "[autoreload of torch.functional failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/functional.py\", line 612, in <module>\n",
      "    \"istft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, \"\n",
      "TypeError: don't know how to add docstring to type 'function'\n",
      "]\n",
      "[autoreload of torch.nn.modules failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/nn/modules/__init__.py\", line 23, in <module>\n",
      "    from .dropout import Dropout, Dropout1d, Dropout2d, Dropout3d, AlphaDropout, FeatureAlphaDropout\n",
      "ImportError: cannot import name 'Dropout1d' from 'torch.nn.modules.dropout' (/ekaterina/env/norppa/lib/python3.7/site-packages/torch/nn/modules/dropout.py)\n",
      "]\n",
      "[autoreload of torch.nn.functional failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/nn/functional.py\", line 78, in <module>\n",
      "    \"\"\",\n",
      "RuntimeError: function 'conv1d' already has a docstring\n",
      "]\n",
      "[autoreload of torch._torch_docs failed: Traceback (most recent call last):\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/ekaterina/env/norppa/lib/python3.7/site-packages/torch/_torch_docs.py\", line 149, in <module>\n",
      "    \"\"\".format(**common_args))\n",
      "RuntimeError: function 'abs' already has a docstring\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "matches, query_label = matches2[0]\n",
    "\n",
    "def get_label(lb):\n",
    "    return lb['labels'][0]['class_id']\n",
    "\n",
    "def get_db_label(lb):\n",
    "    return get_label(lb['db_label'])\n",
    "\n",
    "get_db_label(matches[0])\n",
    "\n",
    "get_label(query_label)\n",
    "\n",
    "wrong_matches = [i for (i, (matches, query_label)) in enumerate(matches2) if not any([get_db_label(match)==get_label(query_label) for match in matches[:5]])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wrong_matches[0][1]\n",
    "\n",
    "# hard_labels = set([get_label(wrong_match[1]) for wrong_match in wrong_matches])\n",
    "\n",
    "# hard_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(wrong_matches)\n",
    "\n",
    "# hard_dataset = [train_dataset[i] for i in wrong_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(wrong_matches, \"wrong_match_inds.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([(img, label) for (img, label) in train_dataset if label['class_id'] in hard_labels])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68004c4925163c4184023d7cef85e997c306b50ccddfa119b94259918fbcddfd"
  },
  "kernelspec": {
   "display_name": "norppa",
   "language": "python",
   "name": "norppa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
